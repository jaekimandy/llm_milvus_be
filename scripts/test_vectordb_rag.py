#!/usr/bin/env python3
"""
Test RAG (Retrieval Augmented Generation) with Specific Information

This script tests the complete RAG pipeline:
1. Retrieves specific information from vector DB
2. Generates response using Claude AI with retrieved context
3. Verifies the LLM correctly uses the retrieved information

Usage:
    python scripts/test_vectordb_rag.py
"""

import sys
import os
import asyncio
import json

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent.vector_store import vector_store
from agent.llm_client import llm_client


async def test_rag_pipeline():
    """Test complete RAG pipeline with single query"""
    print("=" * 70)
    print("ü§ñ Testing RAG Pipeline")
    print("   (Retrieval Augmented Generation)")
    print("=" * 70)
    print()

    test_query = "Where did Jae Kim grow up?"
    print(f"üìù Query: '{test_query}'")
    print()

    # Step 1: Connect to Milvus
    print("1Ô∏è‚É£  Connecting to Milvus...")
    try:
        vector_store.connect()
        from pymilvus import Collection
        vector_store.collection = Collection(vector_store.collection_name)
        count = vector_store.collection.num_entities
        print(f"   ‚úì Connected to Milvus")
        print(f"   ‚úì Documents in database: {count}")
    except Exception as e:
        print(f"   ‚úó Connection failed: {e}")
        sys.exit(1)

    # Step 2: Generate query embedding
    print("\n2Ô∏è‚É£  Generating query embedding...")
    try:
        query_embedding = await llm_client.generate_embeddings(test_query)
        print(f"   ‚úì Generated embedding (dim: {len(query_embedding)})")
        print(f"   ‚úì Provider: {llm_client.embeddings.__class__.__name__}")
    except Exception as e:
        print(f"   ‚úó Failed: {e}")
        sys.exit(1)

    # Step 3: Search vector database
    print("\n3Ô∏è‚É£  Searching vector database...")
    try:
        results = vector_store.search(query_embedding, top_k=3)
        print(f"   ‚úì Found {len(results)} relevant documents\n")

        # Display results
        print("   üìÑ Retrieved Documents:")
        print("   " + "-" * 66)

        found_jae_kim = False
        for i, result in enumerate(results, 1):
            metadata = json.loads(result['metadata'])
            text_preview = result['text'][:70] + "..." if len(result['text']) > 70 else result['text']

            print(f"\n   Document {i}:")
            print(f"      Distance: {result['distance']:.4f}")
            print(f"      Source: {metadata.get('source', 'unknown')}")
            print(f"      Text: {text_preview}")

            # Check if this is our Jae Kim information
            if "Jae Kim" in result['text'] and "Jamsil" in result['text']:
                print(f"      üéØ Contains Jae Kim information!")
                found_jae_kim = True

        print("\n   " + "-" * 66)

        if not found_jae_kim:
            print("\n   ‚ö†Ô∏è  Warning: Jae Kim information not in top 3 results")
            print("   üí° The test information might not have been added yet.")
            print("   üí° Run: python scripts/test_vectordb_specific.py first")
            return

    except Exception as e:
        print(f"   ‚úó Search failed: {e}")
        sys.exit(1)

    # Step 4: Build context from retrieved documents
    print("\n4Ô∏è‚É£  Building context from retrieved documents...")
    context_parts = []
    for i, result in enumerate(results, 1):
        context_parts.append(f"[Document {i}]\n{result['text']}")

    context = "\n\n".join(context_parts)
    print(f"   ‚úì Built context from {len(results)} documents")
    print(f"   ‚úì Total context length: {len(context)} characters")

    # Step 5: Generate response with Claude AI
    print("\n5Ô∏è‚É£  Generating response with Claude AI...")
    try:
        prompt = f"""You are a helpful assistant. Answer the question based ONLY on the provided context.

Context:
{context}

Question: {test_query}

Instructions:
- Use only information from the context above
- If the context contains relevant information, provide a detailed answer
- If the context doesn't contain relevant information, say so
- Be specific and mention exact details from the context

Answer:"""

        print(f"   Sending prompt to Claude...")
        print(f"   Provider: {llm_client.provider}")

        # Format as message for the LLM client
        messages = [{"role": "user", "content": prompt}]
        response = await llm_client.generate_response(messages)
        print(f"   ‚úì Response generated ({len(response)} characters)\n")

        # Display response
        print("   " + "=" * 66)
        print("   ü§ñ Claude's Response:")
        print("   " + "=" * 66)
        print()
        for line in response.split('\n'):
            print(f"   {line}")
        print()
        print("   " + "=" * 66)

    except Exception as e:
        print(f"   ‚úó LLM generation failed: {e}")
        sys.exit(1)

    # Step 6: Verify response quality
    print("\n6Ô∏è‚É£  Verifying response quality...")

    key_terms = {
        "Jamsil": "Jamsil" in response,
        "New York": "New York" in response or "NY" in response,
        "Gyeongju": "Gyeongju" in response or "Gyeong" in response,
        "Kim": "Kim" in response
    }

    found_count = sum(key_terms.values())
    total_terms = len(key_terms)

    print(f"   Key information check:")
    for term, found in key_terms.items():
        status = "‚úì" if found else "‚úó"
        print(f"      {status} {term}: {'Found' if found else 'Not found'}")

    print()
    print(f"   Score: {found_count}/{total_terms} key terms found")

    if found_count >= 3:
        print("   ‚úÖ EXCELLENT: Claude correctly used the retrieved context!")
    elif found_count >= 2:
        print("   ‚ö†Ô∏è  GOOD: Claude used most of the context")
    elif found_count >= 1:
        print("   ‚ö†Ô∏è  PARTIAL: Claude used some context")
    else:
        print("   ‚ùå POOR: Claude didn't use the retrieved context")

    # Final summary
    print("\n" + "=" * 70)
    print("üìä RAG Pipeline Test Summary")
    print("=" * 70)
    print(f"   ‚úì Vector search: Working")
    print(f"   ‚úì Retrieved Jae Kim info: {'Yes' if found_jae_kim else 'No'}")
    print(f"   ‚úì Claude generation: Working")
    print(f"   ‚úì Context usage: {found_count}/{total_terms} terms")
    print()

    if found_jae_kim and found_count >= 2:
        print("   ‚úÖ RAG pipeline is working correctly!")
        print()
        print("   The system successfully:")
        print("   1. Stored specific information in vector DB")
        print("   2. Retrieved it via semantic search")
        print("   3. Used it to answer questions with Claude AI")
    else:
        print("   ‚ö†Ô∏è  RAG pipeline needs improvement")

    print("=" * 70)


async def main():
    """Main test runner"""
    try:
        await test_rag_pipeline()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
